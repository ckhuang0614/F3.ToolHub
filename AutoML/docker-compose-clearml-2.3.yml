version: "3.8"

# Internal ClearML endpoints used by containers (gateway/agent/tasks) on the compose network.
# Notes:
# - UI from your browser is still: http://localhost:8080
# - Fileserver from your browser is still: http://localhost:8081
x-clearml-env: &clearml-env
  CLEARML_API_HOST: http://apiserver:8008
  CLEARML_WEB_HOST: http://webserver
  CLEARML_FILES_HOST: http://fileserver:8081
  CLEARML_DEFAULT_OUTPUT_URI: ${CLEARML_DEFAULT_OUTPUT_URI:-s3://minio:9000/models}

services:
  elasticsearch:
    container_name: clearml-elastic
    image: elasticsearch:8.17.0
    restart: unless-stopped
    environment:
      bootstrap.memory_lock: "true"
      cluster.name: clearml
      cluster.routing.allocation.node_initial_primaries_recoveries: "500"
      cluster.routing.allocation.disk.watermark.low: 500mb
      cluster.routing.allocation.disk.watermark.high: 500mb
      cluster.routing.allocation.disk.watermark.flood_stage: 500mb
      discovery.type: "single-node"
      http.compression_level: "7"
      node.name: clearml
      reindex.remote.whitelist: "'*.*'"
      xpack.security.enabled: "false"
      # If ES memory is tight, you can override:
      # ES_JAVA_OPTS: "-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - es-data:/usr/share/elasticsearch/data
    # Security note: avoid exposing ES to LAN/WAN.
    # ports:
    #   - "9200:9200"

  mongo:
    container_name: clearml-mongo
    image: mongo:7.0.22
    restart: unless-stopped
    command: --setParameter internalQueryMaxBlockingSortMemoryUsageBytes=196100200
    volumes:
      - mongo-data:/data/db
      - mongo-config:/data/configdb
    # ports:
    #   - "27017:27017"

  redis:
    container_name: clearml-redis
    image: redis:7.4.1
    restart: unless-stopped
    volumes:
      - redis-data:/data
    # ports:
    #   - "6379:6379"

  apiserver:
    container_name: clearml-apiserver
    image: clearml/server:2.3.0
    restart: unless-stopped
    command: ["apiserver"]
    depends_on:
      - redis
      - mongo
      - elasticsearch
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      CLEARML_SERVER_DEPLOYMENT_TYPE: linux

      # Pre-populate (first boot) and enable async-delete service.
      CLEARML__apiserver__pre_populate__enabled: "true"
      CLEARML__apiserver__pre_populate__zip_files: "/opt/clearml/db-pre-populate"
      CLEARML__apiserver__pre_populate__artifacts_path: "/mnt/fileserver"
      CLEARML__services__async_urls_delete__enabled: "true"
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"

      # Keep your existing access key/secret (used by gateway/agent) so you don't have to re-init credentials.
      CLEARML__secure__credentials__services_agent__user_key: ${CLEARML_API_ACCESS_KEY:-P4BMJA7RK3TKBXGSY8OAA1FA8TOD11}
      CLEARML__secure__credentials__services_agent__user_secret: ${CLEARML_API_SECRET_KEY:-9LsgSfa0SYz0zli1_c500ZcLqanre2xkWOpepyt1w-BKK3_DKPHrtoj3JSHvyy8bIi0}

    volumes:
      - fileserver-data:/mnt/fileserver
      - ./infra/clearml/storage_credentials.conf:/opt/clearml/config/services/storage_credentials.conf:ro
    ports:
      - "8008:8008"
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8008/debug.ping', timeout=2).read()"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 20s

  webserver:
    container_name: clearml-webserver
    image: clearml/server:2.3.0
    restart: unless-stopped
    command: ["webserver"]
    depends_on:
      - apiserver
    environment:
      - NGINX_APISERVER_ADDRESS=http://apiserver:8008
      - NGINX_FILESERVER_ADDRESS=http://fileserver:8081
    ports:
      - "8080:80"

  fileserver:
    container_name: clearml-fileserver
    image: clearml/server:2.3.0
    restart: unless-stopped
    command: ["fileserver"]
    depends_on:
      apiserver:
        condition: service_healthy
    environment:
      CLEARML__fileserver__auth__enabled: "false"
      CLEARML__fileserver__delete__allow_batch: "true"
    volumes:
      - fileserver-data:/mnt/fileserver
    ports:
      - "8081:8081"

  async_delete:
    container_name: clearml-async-delete
    image: clearml/server:2.3.0
    restart: unless-stopped
    depends_on:
      - apiserver
      - redis
      - mongo
      - elasticsearch
      - fileserver
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      PYTHONPATH: /opt/clearml/apiserver
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
    entrypoint:
      - python3
      - -m
      - jobs.async_urls_delete
      - --fileserver-host
      - http://fileserver:8081
    volumes:
      - clearml-logs:/var/log/clearml
      - ./infra/clearml/storage_credentials.conf:/opt/clearml/config/services/storage_credentials.conf:ro

  # --- Your existing object storage (optional) ---
  minio:
    image: bitnamilegacy/minio:2024
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
      - MINIO_DEFAULT_BUCKETS=datasets:public,models
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9000/minio/health/ready >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    restart: unless-stopped
    profiles: ["monitoring"]
    environment:
      ZOOKEEPER_CLIENT_PORT: "2181"
      ZOOKEEPER_TICK_TIME: "2000"
    volumes:
      - zookeeper-data:/var/lib/zookeeper

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    restart: unless-stopped
    profiles: ["monitoring"]
    environment:
      KAFKA_BROKER_ID: "1"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    depends_on:
      - zookeeper
    volumes:
      - kafka-data:/var/lib/kafka/data

  # --- Your project services (unchanged) ---
  gateway:
    build:
      context: .
      dockerfile: gateway/Dockerfile
    volumes:
      - ./dataset:/data:ro
      - ./infra/clearml/clearml.conf:/etc/clearml/clearml.conf:ro
      - ./infra/clearml/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro
    environment:
      <<: *clearml-env
      CLEARML_API_ACCESS_KEY: ${CLEARML_API_ACCESS_KEY:-P4BMJA7RK3TKBXGSY8OAA1FA8TOD11}
      CLEARML_API_SECRET_KEY: ${CLEARML_API_SECRET_KEY:-9LsgSfa0SYz0zli1_c500ZcLqanre2xkWOpepyt1w-BKK3_DKPHrtoj3JSHvyy8bIi0}
      CLEARML_CONFIG_FILE: /etc/clearml/clearml.conf
      TRAINS_CONFIG_FILE: /etc/clearml/clearml.conf
      CLEARML_DEFAULT_QUEUE: ${CLEARML_DEFAULT_QUEUE:-default}
      CLEARML_QUEUE_AUTOGLOUON: ${CLEARML_QUEUE_AUTOGLOUON:-cpu}
      CLEARML_QUEUE_FLAML: ${CLEARML_QUEUE_FLAML:-cpu}
      CLEARML_QUEUE_ULTRALYTICS: ${CLEARML_QUEUE_ULTRALYTICS:-gpu}
      CLEARML_SERVING_TASK_ID: ${CLEARML_SERVING_TASK_ID:-}
      CLEARML_SERVING_PROJECT: ${CLEARML_SERVING_PROJECT:-DevOps}
      CLEARML_SERVING_NAME: ${CLEARML_SERVING_NAME:-AutoML Serving}
      CLEARML_SERVING_TAGS: ${CLEARML_SERVING_TAGS:-}
      AUTOGLOUON_IMAGE: ${AUTOGLOUON_IMAGE:-f3.autogluon-trainer:latest}
      FLAML_IMAGE: ${FLAML_IMAGE:-f3.flaml-trainer:latest}
      ULTRALYTICS_IMAGE: ${ULTRALYTICS_IMAGE:-f3.ultralytics-trainer:latest}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minioadmin}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
      AWS_ENDPOINT_URL: ${AWS_ENDPOINT_URL:-http://minio:9000}
    ports:
      - "8000:8000"
    depends_on:
      apiserver:
        condition: service_healthy
      webserver:
        condition: service_started
      fileserver:
        condition: service_started
      minio:
        condition: service_healthy

  clearml-serving:
    build:
      context: ./infra/clearml-serving
      dockerfile: Dockerfile
    image: ${CLEARML_SERVING_IMAGE:-f3.clearml-serving:latest}
    restart: unless-stopped
    profiles: ["serving"]
    environment:
      <<: *clearml-env
      CLEARML_API_ACCESS_KEY: ${CLEARML_API_ACCESS_KEY:-P4BMJA7RK3TKBXGSY8OAA1FA8TOD11}
      CLEARML_API_SECRET_KEY: ${CLEARML_API_SECRET_KEY:-9LsgSfa0SYz0zli1_c500ZcLqanre2xkWOpepyt1w-BKK3_DKPHrtoj3JSHvyy8bIi0}
      CLEARML_CONFIG_FILE: /etc/clearml/clearml.conf
      TRAINS_CONFIG_FILE: /etc/clearml/clearml.conf
      CLEARML_SERVING_TASK_ID: ${CLEARML_SERVING_TASK_ID:-}
      CLEARML_SERVING_POLL_FREQ: ${CLEARML_SERVING_POLL_FREQ:-1}
      CLEARML_DEFAULT_KAFKA_SERVE_URL: ${CLEARML_DEFAULT_KAFKA_SERVE_URL:-kafka:9092}
      CLEARML_EXTRA_PYTHON_PACKAGES: ${CLEARML_EXTRA_PYTHON_PACKAGES:-}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minioadmin}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
      AWS_ENDPOINT_URL: ${AWS_ENDPOINT_URL:-http://minio:9000}
    volumes:
      - ./infra/clearml/clearml.conf:/etc/clearml/clearml.conf:ro
      - ./infra/clearml/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro
      - ./dataset:/data:ro
    ports:
      - "8082:8080"
    depends_on:
      apiserver:
        condition: service_healthy
      minio:
        condition: service_healthy

  clearml-serving-stats:
    image: ${CLEARML_SERVING_IMAGE:-f3.clearml-serving:latest}
    restart: unless-stopped
    profiles: ["monitoring"]
    environment:
      <<: *clearml-env
      CLEARML_API_ACCESS_KEY: ${CLEARML_API_ACCESS_KEY:-P4BMJA7RK3TKBXGSY8OAA1FA8TOD11}
      CLEARML_API_SECRET_KEY: ${CLEARML_API_SECRET_KEY:-9LsgSfa0SYz0zli1_c500ZcLqanre2xkWOpepyt1w-BKK3_DKPHrtoj3JSHvyy8bIi0}
      CLEARML_CONFIG_FILE: /etc/clearml/clearml.conf
      TRAINS_CONFIG_FILE: /etc/clearml/clearml.conf
      CLEARML_SERVING_TASK_ID: ${CLEARML_SERVING_TASK_ID:-}
      CLEARML_DEFAULT_KAFKA_SERVE_URL: ${CLEARML_DEFAULT_KAFKA_SERVE_URL:-kafka:9092}
      CLEARML_SERVING_PORT: ${CLEARML_SERVING_STATS_PORT:-9999}
    volumes:
      - ./infra/clearml/clearml.conf:/etc/clearml/clearml.conf:ro
      - ./infra/clearml/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro
    entrypoint: ["python", "-m", "clearml_serving.statistics.main"]
    ports:
      - "${CLEARML_SERVING_STATS_PORT:-9999}:9999"
    depends_on:
      apiserver:
        condition: service_healthy
      kafka:
        condition: service_started

  prometheus:
    image: prom/prometheus:v2.54.1
    restart: unless-stopped
    profiles: ["monitoring"]
    command:
      - --config.file=/etc/prometheus/prometheus.yml
    volumes:
      - ./infra/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    depends_on:
      - clearml-serving-stats

  grafana:
    image: grafana/grafana:11.1.0
    restart: unless-stopped
    profiles: ["monitoring"]
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

  clearml-agent:
    build:
      context: ./infra
      dockerfile: clearml-agent/Dockerfile
    privileged: true
    environment:
      <<: *clearml-env
      CLEARML_API_ACCESS_KEY: ${CLEARML_API_ACCESS_KEY:-P4BMJA7RK3TKBXGSY8OAA1FA8TOD11}
      CLEARML_API_SECRET_KEY: ${CLEARML_API_SECRET_KEY:-9LsgSfa0SYz0zli1_c500ZcLqanre2xkWOpepyt1w-BKK3_DKPHrtoj3JSHvyy8bIi0}
      CLEARML_CONFIG_FILE: /etc/clearml/clearml.conf
      TRAINS_CONFIG_FILE: /etc/clearml/clearml.conf
      CLEARML_AGENT_DOCKER_RUNTIME: ${CLEARML_AGENT_DOCKER_RUNTIME:-}
      # Make the default network robust (uses COMPOSE_PROJECT_NAME; falls back to "automl")
      CLEARML_AGENT_EXTRA_DOCKER_ARGS: ${CLEARML_AGENT_EXTRA_DOCKER_ARGS:---network ${COMPOSE_PROJECT_NAME:-automl}_default -e CLEARML_API_ACCESS_KEY -e CLEARML_API_SECRET_KEY -e CLEARML_API_HOST -e CLEARML_WEB_HOST -e CLEARML_FILES_HOST -e CLEARML_CONFIG_FILE=/etc/clearml/clearml.conf -e TRAINS_CONFIG_FILE=/etc/clearml/clearml.conf -e AWS_ACCESS_KEY_ID=minioadmin -e AWS_SECRET_ACCESS_KEY=minioadmin -e AWS_DEFAULT_REGION=us-east-1 -e AWS_ENDPOINT_URL=http://minio:9000 -e CLEARML__sdk__aws__s3__host=minio:9000 -e CLEARML__sdk__aws__s3__secure=false -e CLEARML__sdk__aws__s3__verify=false -e CLEARML__sdk__aws__s3__use_credentials_chain=false -e CLEARML__sdk__aws__s3__region=us-east-1 -e CLEARML_PATCH_PYTORCH=0 -e CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL=1 -v ${CLEARML_CONFIG_HOST_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/infra/clearml}/clearml.conf:/etc/clearml/clearml.conf:ro -v ${CLEARML_CONFIG_HOST_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/infra/clearml}/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro --shm-size=1g}
      CLEARML_WORKER_NAME: docker-agent
      CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL: "true"
      CLEARML_AGENT_USE_LEGACY_HASH: "true"
      CLEARML_LOG_LEVEL: INFO
      CLEARML_AGENT_DEFAULT_QUEUE: ${CLEARML_DEFAULT_QUEUE:-default}
      CLEARML_AGENT_TEMP_STDOUT_FILE_DIR: ${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}
    volumes:
      - ./infra/clearml/clearml.conf:/etc/clearml/clearml.conf:ro
      - ./infra/clearml/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro
      - ./.clearml-host:/clearml_host
      - ${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}:${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      apiserver:
        condition: service_healthy
      webserver:
        condition: service_started
      fileserver:
        condition: service_started
      minio:
        condition: service_healthy
    command: ["daemon", "--queue", "${CLEARML_AGENT_DEFAULT_QUEUE:-default}", "--docker", "--foreground"]

  clearml-agent-cpu:
    build:
      context: ./infra
      dockerfile: clearml-agent/Dockerfile
    privileged: true
    environment:
      <<: *clearml-env
      CLEARML_API_ACCESS_KEY: ${CLEARML_API_ACCESS_KEY:-P4BMJA7RK3TKBXGSY8OAA1FA8TOD11}
      CLEARML_API_SECRET_KEY: ${CLEARML_API_SECRET_KEY:-9LsgSfa0SYz0zli1_c500ZcLqanre2xkWOpepyt1w-BKK3_DKPHrtoj3JSHvyy8bIi0}
      CLEARML_CONFIG_FILE: /etc/clearml/clearml.conf
      TRAINS_CONFIG_FILE: /etc/clearml/clearml.conf
      CLEARML_AGENT_DOCKER_RUNTIME: ${CLEARML_AGENT_DOCKER_RUNTIME:-}
      CLEARML_AGENT_EXTRA_DOCKER_ARGS: ${CLEARML_AGENT_EXTRA_DOCKER_ARGS_CPU:---network ${COMPOSE_PROJECT_NAME:-automl}_default -e CLEARML_API_ACCESS_KEY -e CLEARML_API_SECRET_KEY -e CLEARML_API_HOST -e CLEARML_WEB_HOST -e CLEARML_FILES_HOST -e CLEARML_CONFIG_FILE=/etc/clearml/clearml.conf -e TRAINS_CONFIG_FILE=/etc/clearml/clearml.conf -e AWS_ACCESS_KEY_ID=minioadmin -e AWS_SECRET_ACCESS_KEY=minioadmin -e AWS_DEFAULT_REGION=us-east-1 -e AWS_ENDPOINT_URL=http://minio:9000 -e CLEARML__sdk__aws__s3__host=minio:9000 -e CLEARML__sdk__aws__s3__secure=false -e CLEARML__sdk__aws__s3__verify=false -e CLEARML__sdk__aws__s3__use_credentials_chain=false -e CLEARML__sdk__aws__s3__region=us-east-1 -e CLEARML_PATCH_PYTORCH=0 -e CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL=1 -v ${CLEARML_CONFIG_HOST_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/infra/clearml}/clearml.conf:/etc/clearml/clearml.conf:ro -v ${CLEARML_CONFIG_HOST_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/infra/clearml}/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro --shm-size=1g}
      CLEARML_WORKER_NAME: docker-agent-cpu
      CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL: "true"
      CLEARML_AGENT_USE_LEGACY_HASH: "true"
      CLEARML_LOG_LEVEL: INFO
      CLEARML_AGENT_DEFAULT_QUEUE: ${CLEARML_CPU_QUEUE:-cpu}
      CLEARML_AGENT_TEMP_STDOUT_FILE_DIR: ${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}
    volumes:
      - ./infra/clearml/clearml.conf:/etc/clearml/clearml.conf:ro
      - ./infra/clearml/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro
      - ./.clearml-host:/clearml_host
      - ${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}:${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      apiserver:
        condition: service_healthy
      webserver:
        condition: service_started
      fileserver:
        condition: service_started
      minio:
        condition: service_healthy
    command: ["daemon", "--queue", "${CLEARML_AGENT_DEFAULT_QUEUE:-cpu}", "--docker", "--foreground"]

  clearml-agent-gpu:
    build:
      context: ./infra
      dockerfile: clearml-agent/Dockerfile
    privileged: true
    environment:
      <<: *clearml-env
      CLEARML_API_ACCESS_KEY: ${CLEARML_API_ACCESS_KEY:-P4BMJA7RK3TKBXGSY8OAA1FA8TOD11}
      CLEARML_API_SECRET_KEY: ${CLEARML_API_SECRET_KEY:-9LsgSfa0SYz0zli1_c500ZcLqanre2xkWOpepyt1w-BKK3_DKPHrtoj3JSHvyy8bIi0}
      CLEARML_CONFIG_FILE: /etc/clearml/clearml.conf
      TRAINS_CONFIG_FILE: /etc/clearml/clearml.conf
      CLEARML_AGENT_DOCKER_RUNTIME: ${CLEARML_AGENT_DOCKER_RUNTIME_GPU:-nvidia}
      CLEARML_AGENT_EXTRA_DOCKER_ARGS: ${CLEARML_AGENT_EXTRA_DOCKER_ARGS_GPU:---network ${COMPOSE_PROJECT_NAME:-automl}_default -e CLEARML_API_ACCESS_KEY -e CLEARML_API_SECRET_KEY -e CLEARML_API_HOST -e CLEARML_WEB_HOST -e CLEARML_FILES_HOST -e CLEARML_CONFIG_FILE=/etc/clearml/clearml.conf -e TRAINS_CONFIG_FILE=/etc/clearml/clearml.conf -e AWS_ACCESS_KEY_ID=minioadmin -e AWS_SECRET_ACCESS_KEY=minioadmin -e AWS_DEFAULT_REGION=us-east-1 -e AWS_ENDPOINT_URL=http://minio:9000 -e CLEARML__sdk__aws__s3__host=minio:9000 -e CLEARML__sdk__aws__s3__secure=false -e CLEARML__sdk__aws__s3__verify=false -e CLEARML__sdk__aws__s3__use_credentials_chain=false -e CLEARML__sdk__aws__s3__region=us-east-1 -e CLEARML_PATCH_PYTORCH=0 -e CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL=1 -v ${CLEARML_CONFIG_HOST_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/infra/clearml}/clearml.conf:/etc/clearml/clearml.conf:ro -v ${CLEARML_CONFIG_HOST_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/infra/clearml}/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro --gpus all --shm-size=1g}
      CLEARML_WORKER_NAME: docker-agent-gpu
      CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL: "true"
      CLEARML_AGENT_USE_LEGACY_HASH: "true"
      CLEARML_LOG_LEVEL: INFO
      CLEARML_AGENT_DEFAULT_QUEUE: ${CLEARML_GPU_QUEUE:-gpu}
      CLEARML_AGENT_TEMP_STDOUT_FILE_DIR: ${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}
    volumes:
      - ./infra/clearml/clearml.conf:/etc/clearml/clearml.conf:ro
      - ./infra/clearml/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro
      - ./.clearml-host:/clearml_host
      - ${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}:${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      apiserver:
        condition: service_healthy
      webserver:
        condition: service_started
      fileserver:
        condition: service_started
      minio:
        condition: service_healthy
    command: ["daemon", "--queue", "${CLEARML_AGENT_DEFAULT_QUEUE:-gpu}", "--docker", "--foreground"]

  clearml-agent-services:
    build:
      context: ./infra
      dockerfile: clearml-agent/Dockerfile
    privileged: true
    environment:
      <<: *clearml-env
      CLEARML_API_ACCESS_KEY: ${CLEARML_API_ACCESS_KEY:-P4BMJA7RK3TKBXGSY8OAA1FA8TOD11}
      CLEARML_API_SECRET_KEY: ${CLEARML_API_SECRET_KEY:-9LsgSfa0SYz0zli1_c500ZcLqanre2xkWOpepyt1w-BKK3_DKPHrtoj3JSHvyy8bIi0}
      CLEARML_CONFIG_FILE: /etc/clearml/clearml.conf
      TRAINS_CONFIG_FILE: /etc/clearml/clearml.conf
      CLEARML_AGENT_DOCKER_RUNTIME: ${CLEARML_AGENT_DOCKER_RUNTIME:-}
      CLEARML_AGENT_EXTRA_DOCKER_ARGS: ${CLEARML_AGENT_EXTRA_DOCKER_ARGS_SERVICES:---network ${COMPOSE_PROJECT_NAME:-automl}_default -e CLEARML_API_ACCESS_KEY -e CLEARML_API_SECRET_KEY -e CLEARML_API_HOST -e CLEARML_WEB_HOST -e CLEARML_FILES_HOST -e CLEARML_CONFIG_FILE=/etc/clearml/clearml.conf -e TRAINS_CONFIG_FILE=/etc/clearml/clearml.conf -e AWS_ACCESS_KEY_ID=minioadmin -e AWS_SECRET_ACCESS_KEY=minioadmin -e AWS_DEFAULT_REGION=us-east-1 -e AWS_ENDPOINT_URL=http://minio:9000 -e CLEARML__sdk__aws__s3__host=minio:9000 -e CLEARML__sdk__aws__s3__secure=false -e CLEARML__sdk__aws__s3__verify=false -e CLEARML__sdk__aws__s3__use_credentials_chain=false -e CLEARML__sdk__aws__s3__region=us-east-1 -e CLEARML_PATCH_PYTORCH=0 -e CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL=1 -v ${CLEARML_CONFIG_HOST_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/infra/clearml}/clearml.conf:/etc/clearml/clearml.conf:ro -v ${CLEARML_CONFIG_HOST_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/infra/clearml}/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro --shm-size=1g}
      CLEARML_WORKER_NAME: docker-agent-services
      CLEARML_AGENT_SKIP_PYTHON_ENV_INSTALL: "true"
      CLEARML_AGENT_USE_LEGACY_HASH: "true"
      CLEARML_LOG_LEVEL: INFO
      CLEARML_AGENT_DEFAULT_QUEUE: ${CLEARML_SERVICES_QUEUE:-services}
      CLEARML_AGENT_TEMP_STDOUT_FILE_DIR: ${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}
    volumes:
      - ./infra/clearml/clearml.conf:/etc/clearml/clearml.conf:ro
      - ./infra/clearml/storage_credentials.conf:/etc/clearml/storage_credentials.conf:ro
      - ./.clearml-host:/clearml_host
      - ${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}:${CLEARML_AGENT_TEMP_STDOUT_FILE_DIR:-/run/desktop/mnt/host/d/Project/F3.ToolHub/AutoML/.clearml-host}
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      apiserver:
        condition: service_healthy
      webserver:
        condition: service_started
      fileserver:
        condition: service_started
      minio:
        condition: service_healthy
    command: ["daemon", "--queue", "${CLEARML_AGENT_DEFAULT_QUEUE:-services}", "--docker", "--foreground"]

  autogluon-trainer:
    build:
      context: .
      dockerfile: trainers/autogluon/Dockerfile
    image: f3.autogluon-trainer:latest
    profiles: ["build-only"]

  flaml-trainer:
    build:
      context: .
      dockerfile: trainers/flaml/Dockerfile
    image: f3.flaml-trainer:latest
    profiles: ["build-only"]

  ultralytics-trainer:
    build:
      context: .
      dockerfile: trainers/ultralytics/Dockerfile
    image: f3.ultralytics-trainer:latest
    profiles: ["build-only"]

volumes:
  es-data:
  mongo-data:
  mongo-config:
  redis-data:
  minio-data:
  fileserver-data:
  clearml-logs:
  zookeeper-data:
  kafka-data:
  prometheus-data:
  grafana-data:
